{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25008c6b",
   "metadata": {},
   "source": [
    "# Lab 1 — Running Your First Transformer Models \n",
    "\n",
    "Goal: **get everyone running models successfully** on CPU (GPU optional).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c3588",
   "metadata": {},
   "source": [
    "## 0) One-time setup (copy/paste)\n",
    "\n",
    "Open a terminal and run:\n",
    "\n",
    "```bash\n",
    "conda create -n nlp python=3.10 -y\n",
    "conda activate nlp\n",
    "\n",
    "# Pin NumPy to the stable 1.x series to avoid binary-compatibility issues.\n",
    "pip install \"numpy<2\"\n",
    "\n",
    "# Install the core libraries we need.\n",
    "pip install torch transformers accelerate ipykernel\n",
    "\n",
    "# Make this environment show up as a Jupyter kernel.\n",
    "python -m ipykernel install --user --name nlp --display-name \"Python (nlp)\"\n",
    "```\n",
    "\n",
    "Then in Jupyter: **Kernel → Change Kernel → Python (nlp)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07510454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\George\\dev\\COM6513\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0c9e7",
   "metadata": {},
   "source": [
    "## 1) Sentiment analysis (CPU-friendly)\n",
    "\n",
    "We run a pretrained sentiment analysis model on a few example sentences.\n",
    "This should work on CPU.\n",
    "\n",
    "**First:** run the cell below as-is and check that it works.\n",
    "\n",
    "**Then (optional):**\n",
    "- add your own sentences to the list,\n",
    "- try short or ambiguous text (e.g. “meh”, “it was fine”),\n",
    "- observe how the confidence score changes.\n",
    "\n",
    "Note: The first run downloads the model files (one-time).\n",
    "If your network is slow, simply re-run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410f5c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be96d028839f481db597fea9a15363a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I absolutely loved this movie — it was brilliant. -> {'label': 'POSITIVE', 'score': 0.9998853206634521}\n",
      "This was a waste of time and money. -> {'label': 'NEGATIVE', 'score': 0.9997945427894592}\n",
      "The plot was okay, but the acting was amazing. -> {'label': 'POSITIVE', 'score': 0.9998676776885986}\n",
      "Not sure how I feel about it. -> {'label': 'NEGATIVE', 'score': 0.9994102716445923}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"HF_HUB_DOWNLOAD_TIMEOUT\", \"300\")\n",
    "os.environ.setdefault(\"HF_HUB_ETAG_TIMEOUT\", \"60\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    \"I absolutely loved this movie — it was brilliant.\",\n",
    "    \"This was a waste of time and money.\",\n",
    "    \"The plot was okay, but the acting was amazing.\",\n",
    "    \"Not sure how I feel about it.\"\n",
    "]\n",
    "\n",
    "for t in examples:\n",
    "    print(t, \"->\", sentiment(t)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb79a1",
   "metadata": {},
   "source": [
    "## 2) Text generation (small GPT-style model)\n",
    "\n",
    "We run a small decoder-only language model (`distilgpt2`) to generate text continuations.\n",
    "This should work on CPU.\n",
    "\n",
    "**First:** run the cell below as-is and check that it works.\n",
    "\n",
    "**Then (optional):**\n",
    "- change the prompt text,\n",
    "- experiment with the generation parameters:\n",
    "  - `max_new_tokens`: how long the generated continuation is,\n",
    "  - `temperature`: how random vs. predictable the output is,\n",
    "  - `top_p`: how much of the probability mass the model samples from,\n",
    "- observe how the output changes.\n",
    "\n",
    "There are no right or wrong outputs — just explore how the model behaves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c787098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d5a94192a148fe849b9c1ae49f4168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=40) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=40) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROMPT: In Sheffield, the weather today\n",
      "OUTPUT: In Sheffield, the weather today has been unusually pleasant, as the weather has been a bit of a relief and it is still relatively calm.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=40) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROMPT: Neural networks learn representations by\n",
      "OUTPUT: Neural networks learn representations by their neurons (an experiment) to be able to perform an autoreaction of the neuron by the activity of the neuron in the right hemisphere (NHTFC). Thus, the observed neural network was\n",
      "\n",
      "PROMPT: Once upon a time in a city made of steel,\n",
      "OUTPUT: Once upon a time in a city made of steel, that the people of the people of Dukon (now Dukon) would become the first people to have a real opportunity to live in one of the most beautiful cities in the world.�\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"HF_HUB_DOWNLOAD_TIMEOUT\", \"300\")\n",
    "os.environ.setdefault(\"HF_HUB_ETAG_TIMEOUT\", \"60\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"In Sheffield, the weather today\",\n",
    "    \"Neural networks learn representations by\",\n",
    "    \"Once upon a time in a city made of steel,\"\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    out = gen(p, max_new_tokens=40, do_sample=True, top_p=0.95, temperature=0.9)\n",
    "    print(\"\\nPROMPT:\", p)\n",
    "    print(\"OUTPUT:\", out[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d2c841",
   "metadata": {},
   "source": [
    "## 3) Checklist\n",
    "\n",
    "By the end of this lab you should be able to:\n",
    "- create the `nlp` conda environment and select **Python (nlp)** as the kernel\n",
    "- run a transformer sentiment model\n",
    "- run a small text-generation model\n",
    "\n",
    "If something fails, copy the full error message and also paste the output of:\n",
    "```python\n",
    "import sys\n",
    "print(sys.executable)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COM6513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
