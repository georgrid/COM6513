{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25008c6b",
   "metadata": {},
   "source": [
    "# Lab 1 — Running Your First Transformer Models \n",
    "\n",
    "Goal: **get everyone running models successfully** on CPU (GPU optional).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c3588",
   "metadata": {},
   "source": [
    "## 0) One-time setup (copy/paste)\n",
    "\n",
    "Open a terminal and run:\n",
    "\n",
    "```bash\n",
    "conda create -n nlp python=3.10 -y\n",
    "conda activate nlp\n",
    "\n",
    "# Pin NumPy to the stable 1.x series to avoid binary-compatibility issues.\n",
    "pip install \"numpy<2\"\n",
    "\n",
    "# Install the core libraries we need.\n",
    "pip install torch transformers accelerate ipykernel\n",
    "\n",
    "# Make this environment show up as a Jupyter kernel.\n",
    "python -m ipykernel install --user --name nlp --display-name \"Python (nlp)\"\n",
    "```\n",
    "\n",
    "Then in Jupyter: **Kernel → Change Kernel → Python (nlp)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07510454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nafisesadatmoosavi/miniconda3/envs/nlp/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0c9e7",
   "metadata": {},
   "source": [
    "## 1) Sentiment analysis (CPU-friendly)\n",
    "\n",
    "We run a pretrained sentiment analysis model on a few example sentences.\n",
    "This should work on CPU.\n",
    "\n",
    "**First:** run the cell below as-is and check that it works.\n",
    "\n",
    "**Then (optional):**\n",
    "- add your own sentences to the list,\n",
    "- try short or ambiguous text (e.g. “meh”, “it was fine”),\n",
    "- observe how the confidence score changes.\n",
    "\n",
    "Note: The first run downloads the model files (one-time).\n",
    "If your network is slow, simply re-run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410f5c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nafisesadatmoosavi/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|█| 104/104 [00:00<00:00, 818.96it/s, Materializing param=p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I absolutely loved this movie — it was brilliant. -> {'label': 'POSITIVE', 'score': 0.9998853206634521}\n",
      "This was a waste of time and money. -> {'label': 'NEGATIVE', 'score': 0.9997945427894592}\n",
      "The plot was okay, but the acting was amazing. -> {'label': 'POSITIVE', 'score': 0.9998676776885986}\n",
      "Not sure how I feel about it. -> {'label': 'NEGATIVE', 'score': 0.9994102716445923}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"HF_HUB_DOWNLOAD_TIMEOUT\", \"300\")\n",
    "os.environ.setdefault(\"HF_HUB_ETAG_TIMEOUT\", \"60\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    \"I absolutely loved this movie — it was brilliant.\",\n",
    "    \"This was a waste of time and money.\",\n",
    "    \"The plot was okay, but the acting was amazing.\",\n",
    "    \"Not sure how I feel about it.\"\n",
    "]\n",
    "\n",
    "for t in examples:\n",
    "    print(t, \"->\", sentiment(t)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb79a1",
   "metadata": {},
   "source": [
    "## 2) Text generation (small GPT-style model)\n",
    "\n",
    "We run a small decoder-only language model (`distilgpt2`) to generate text continuations.\n",
    "This should work on CPU.\n",
    "\n",
    "**First:** run the cell below as-is and check that it works.\n",
    "\n",
    "**Then (optional):**\n",
    "- change the prompt text,\n",
    "- experiment with the generation parameters:\n",
    "  - `max_new_tokens`: how long the generated continuation is,\n",
    "  - `temperature`: how random vs. predictable the output is,\n",
    "  - `top_p`: how much of the probability mass the model samples from,\n",
    "- observe how the output changes.\n",
    "\n",
    "There are no right or wrong outputs — just explore how the model behaves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c787098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 76/76 [00:00<00:00, 915.97it/s, Materializing param=tra\n",
      "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Passing `generation_config` together with generation-related arguments=({'temperature', 'max_new_tokens', 'top_p', 'do_sample'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=40) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=40) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROMPT: In Sheffield, the weather today\n",
      "OUTPUT: In Sheffield, the weather today was perfect. This is a great day in Sheffield for anyone to experience.\n",
      "\n",
      "\n",
      "The weather was perfect.\n",
      "We all remember the last time we went out in the rain and then our rain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=40) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROMPT: Neural networks learn representations by\n",
      "OUTPUT: Neural networks learn representations by an algorithm. The task of finding a network is to find a network (in terms of the number of nodes that are connected to the network). The task of finding a network is to find a network\n",
      "\n",
      "PROMPT: Once upon a time in a city made of steel,\n",
      "OUTPUT: Once upon a time in a city made of steel, there were those who knew the world at large and who knew the world and knew the world. They were all there.\n",
      "\n",
      "\n",
      "The New Age, the Old Age, and the New Age were\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"HF_HUB_DOWNLOAD_TIMEOUT\", \"300\")\n",
    "os.environ.setdefault(\"HF_HUB_ETAG_TIMEOUT\", \"60\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"In Sheffield, the weather today\",\n",
    "    \"Neural networks learn representations by\",\n",
    "    \"Once upon a time in a city made of steel,\"\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    out = gen(p, max_new_tokens=40, do_sample=True, top_p=0.95, temperature=0.9)\n",
    "    print(\"\\nPROMPT:\", p)\n",
    "    print(\"OUTPUT:\", out[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d2c841",
   "metadata": {},
   "source": [
    "## 3) Checklist\n",
    "\n",
    "By the end of this lab you should be able to:\n",
    "- create the `nlp` conda environment and select **Python (nlp)** as the kernel\n",
    "- run a transformer sentiment model\n",
    "- run a small text-generation model\n",
    "\n",
    "If something fails, copy the full error message and also paste the output of:\n",
    "```python\n",
    "import sys\n",
    "print(sys.executable)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
