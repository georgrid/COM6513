{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Tokenization and N-gram Language Models \n",
    "\n",
    "## Why Are We Doing This?\n",
    "\n",
    "In lectures, you learned that:\n",
    "- Tokenization determines how text is represented.\n",
    "- Language models assign probabilities to token sequences.\n",
    "- Perplexity depends on vocabulary choice and sparsity.\n",
    "\n",
    "In this lab, you will connect these ideas *empirically*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Guide: Hugging Face Tokenizers\n",
    "\n",
    "Load a tokenizer:\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "```\n",
    "\n",
    "Tokenize text:\n",
    "```python\n",
    "tokens = tokenizer.tokenize(text)\n",
    "```\n",
    "\n",
    "We will compare:\n",
    "- GPT-2 → Byte Pair Encoding (BPE)\n",
    "- BERT → WordPiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Load Tokenizers\n",
    "\n",
    "Hint:\n",
    "```python\n",
    "AutoTokenizer.from_pretrained('gpt2')\n",
    "AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load GPT-2 tokenizer\n",
    "gpt2 = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# TODO: Load BERT tokenizer\n",
    "bert = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Tokenize Example Sentences\n",
    "\n",
    "Goal: Observe how BPE and WordPiece split words differently.\n",
    "\n",
    "For each sentence:\n",
    "- Tokenize using GPT-2\n",
    "- Tokenize using BERT\n",
    "- Print tokens and their counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are powerful models. \n",
      "GPT-2: ['Transform', 'ers', 'Ġare', 'Ġpowerful', 'Ġmodels', '.'] \n",
      "BERT: ['transformers', 'are', 'powerful', 'models', '.'] \n",
      "\n",
      "Unbelievable tokenization differences! \n",
      "GPT-2: ['Un', 'bel', 'iev', 'able', 'Ġtoken', 'ization', 'Ġdifferences', '!'] \n",
      "BERT: ['unbelievable', 'token', '##ization', 'differences', '!'] \n",
      "\n",
      "supercalifragilisticexpialidocious \n",
      "GPT-2: ['super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious'] \n",
      "BERT: ['super', '##cal', '##if', '##rag', '##ilis', '##tic', '##ex', '##pia', '##lid', '##oc', '##ious'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Transformers are powerful models.\",\n",
    "    \"Unbelievable tokenization differences!\",\n",
    "    \"supercalifragilisticexpialidocious\"\n",
    "]\n",
    "\n",
    "# TODO: Tokenize and print results\n",
    "for i in range(3):\n",
    "    print(f'{sentences[i]} \\nGPT-2: {gpt2.tokenize(sentences[i])} \\nBERT: {bert.tokenize(sentences[i])} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Load Dataset\n",
    "\n",
    "We will train a simple bigram language model.\n",
    "\n",
    "Hint:\n",
    "```python\n",
    "dataset = load_dataset('ag_news', split='train[:200]')\n",
    "```\n",
    "\n",
    "Use first 150 as train and last 50 as test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load dataset and split train/test\n",
    "dataset_train = load_dataset('ag_news', split = 'train[:150]')\n",
    "dataset_test = load_dataset('ag_news', split = 'test[:50]')\n",
    "\n",
    "training = dataset_train['text']\n",
    "test = dataset_test['text']\n",
    "\n",
    "print(training[1])\n",
    "print(test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Tokenize Dataset\n",
    "\n",
    "Write a function that:\n",
    "\n",
    "- Takes a list of raw text strings\n",
    "- Returns a list of token lists (one list of tokens per sentence)\n",
    "\n",
    "After writing the function, you MUST apply it to the training and test sets using the GPT-2 and BERT tokenizers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer):\n",
    "    tokenized = []\n",
    "    \n",
    "    # Hint: loop over texts and use tokenizer.tokenize(text)\n",
    "    for text in texts:\n",
    "        tokenized.append(tokenizer.tokenize(text))\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "training_gpt2 = tokenize_texts(training, gpt2)\n",
    "test_gpt2 = tokenize_texts(test, gpt2)\n",
    "\n",
    "training_bert = tokenize_texts(training, bert)\n",
    "test_bert = tokenize_texts(test, bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Vocabulary & Sparsity Analysis\n",
    "- Compute vocabulary size\n",
    "- Compute average sentence length\n",
    "- Compute singleton rate (tokens appearing once)\n",
    "\n",
    "Think: What does singleton rate tell us about sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(tokenized_texts):\n",
    "    vocab = set()\n",
    "    # TODO: update vocabulary with tokens\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleton_rate(tokenized_texts):\n",
    "    # Hint:\n",
    "    # 1. Flatten tokens\n",
    "    # 2. Count using Counter\n",
    "    # 3. Count how many appear once\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Build Bigram Language Model\n",
    "\n",
    "Bigram probability:\n",
    "P(w_i | w_{i-1}) = Count(w_{i-1}, w_i) / Count(w_{i-1})\n",
    "\n",
    "You must:\n",
    "- Count unigrams\n",
    "- Count bigrams\n",
    "\n",
    "You must build TWO separate bigram models:\n",
    "\n",
    "1. One using GPT-2 tokens\n",
    "2. One using BERT tokens\n",
    "\n",
    "\n",
    "- Use ONLY the TRAINING set to compute:\n",
    "  - Unigram counts\n",
    "  - Bigram counts\n",
    "\n",
    "- Use the TEST set ONLY to compute perplexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_counts(tokenized_texts):\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = defaultdict(Counter)\n",
    "    \n",
    "    # TODO: implement counting\n",
    "    \n",
    "    return unigram_counts, bigram_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(tokenized_texts, unigram_counts, bigram_counts):\n",
    "    log_prob = 0\n",
    "    N = 0\n",
    "    \n",
    "    # TODO: compute log probabilities\n",
    "    \n",
    "    return math.exp(-log_prob / N)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "com6513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
