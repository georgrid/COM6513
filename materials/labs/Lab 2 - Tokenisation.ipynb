{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Tokenization and N-gram Language Models \n",
    "\n",
    "## Why Are We Doing This?\n",
    "\n",
    "In lectures, you learned that:\n",
    "- Tokenization determines how text is represented.\n",
    "- Language models assign probabilities to token sequences.\n",
    "- Perplexity depends on vocabulary choice and sparsity.\n",
    "\n",
    "In this lab, you will connect these ideas *empirically*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Guide: Hugging Face Tokenizers\n",
    "\n",
    "Load a tokenizer:\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "```\n",
    "\n",
    "Tokenize text:\n",
    "```python\n",
    "tokens = tokenizer.tokenize(text)\n",
    "```\n",
    "\n",
    "We will compare:\n",
    "- GPT-2 → Byte Pair Encoding (BPE)\n",
    "- BERT → WordPiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Load Tokenizers\n",
    "\n",
    "Hint:\n",
    "```python\n",
    "AutoTokenizer.from_pretrained('gpt2')\n",
    "AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load GPT-2 tokenizer\n",
    "gpt2 = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# TODO: Load BERT tokenizer\n",
    "bert = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Tokenize Example Sentences\n",
    "\n",
    "Goal: Observe how BPE and WordPiece split words differently.\n",
    "\n",
    "For each sentence:\n",
    "- Tokenize using GPT-2\n",
    "- Tokenize using BERT\n",
    "- Print tokens and their counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are powerful models. \n",
      "GPT-2: ['Transform', 'ers', 'Ġare', 'Ġpowerful', 'Ġmodels', '.'] \n",
      "BERT: ['transformers', 'are', 'powerful', 'models', '.'] \n",
      "\n",
      "Unbelievable tokenization differences! \n",
      "GPT-2: ['Un', 'bel', 'iev', 'able', 'Ġtoken', 'ization', 'Ġdifferences', '!'] \n",
      "BERT: ['unbelievable', 'token', '##ization', 'differences', '!'] \n",
      "\n",
      "supercalifragilisticexpialidocious \n",
      "GPT-2: ['super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious'] \n",
      "BERT: ['super', '##cal', '##if', '##rag', '##ilis', '##tic', '##ex', '##pia', '##lid', '##oc', '##ious'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Transformers are powerful models.\",\n",
    "    \"Unbelievable tokenization differences!\",\n",
    "    \"supercalifragilisticexpialidocious\"\n",
    "]\n",
    "\n",
    "# TODO: Tokenize and print results\n",
    "for i in range(3):\n",
    "    print(f'{sentences[i]} \\nGPT-2: {gpt2.tokenize(sentences[i])} \\nBERT: {bert.tokenize(sentences[i])} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Load Dataset\n",
    "\n",
    "We will train a simple bigram language model.\n",
    "\n",
    "Hint:\n",
    "```python\n",
    "dataset = load_dataset('ag_news', split='train[:200]')\n",
    "```\n",
    "\n",
    "Use first 150 as train and last 50 as test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load dataset and split train/test\n",
    "dataset_train = load_dataset('ag_news', split = 'train[:150]')\n",
    "dataset_test = load_dataset('ag_news', split = 'test[:50]')\n",
    "\n",
    "training = dataset_train['text']\n",
    "test = dataset_test['text']\n",
    "\n",
    "print(training[0])\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Tokenize Dataset\n",
    "\n",
    "Write a function that:\n",
    "\n",
    "- Takes a list of raw text strings\n",
    "- Returns a list of token lists (one list of tokens per sentence)\n",
    "\n",
    "After writing the function, you MUST apply it to the training and test sets using the GPT-2 and BERT tokenizers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer):\n",
    "    tokenized = []\n",
    "    \n",
    "    # Hint: loop over texts and use tokenizer.tokenize(text)\n",
    "    for text in texts:\n",
    "        tokenized.append(tokenizer.tokenize(text))\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "training_gpt2 = tokenize_texts(training, gpt2)\n",
    "test_gpt2 = tokenize_texts(test, gpt2)\n",
    "\n",
    "training_bert = tokenize_texts(training, bert)\n",
    "test_bert = tokenize_texts(test, bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Vocabulary & Sparsity Analysis\n",
    "- Compute vocabulary size\n",
    "- Compute average sentence length\n",
    "- Compute singleton rate (tokens appearing once)\n",
    "\n",
    "Think: What does singleton rate tell us about sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(tokenized_texts):\n",
    "    vocab = set()\n",
    "\n",
    "    # TODO: update vocabulary with tokens\n",
    "    for text in tokenized_texts:\n",
    "        for token in text:\n",
    "            vocab.add(token)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_sentence_length(tokenized_texts):\n",
    "    \n",
    "    lens = []\n",
    "    for text in tokenized_texts:\n",
    "        lens.append(len(text))\n",
    "    \n",
    "    return round(np.sum(lens)/ len(lens), 1)\n",
    "\n",
    "avg_sentence_length(training_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2088"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def singleton_rate(tokenized_texts):\n",
    "    # Hint:\n",
    "    # 1. Flatten tokens\n",
    "    # 2. Count using Counter\n",
    "    # 3. Count how many appear once\n",
    "    all_tokens = []\n",
    "    singletons = []\n",
    "\n",
    "    for i in range(len(tokenized_texts)):\n",
    "        all_tokens += tokenized_texts[i]\n",
    "    \n",
    "    counts = Counter(all_tokens)\n",
    "    for token, count in counts.items():\n",
    "        if count == 1:\n",
    "            singletons.append(token)\n",
    "\n",
    "    return len(singletons)\n",
    "        \n",
    "singleton_rate(training_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Build Bigram Language Model\n",
    "\n",
    "Bigram probability:\n",
    "$P(w_i | w_{i-1}) = Count(w_{i-1}, w_i) / Count(w_{i-1})$\n",
    "\n",
    "You must:\n",
    "- Count unigrams\n",
    "- Count bigrams\n",
    "\n",
    "You must build TWO separate bigram models:\n",
    "\n",
    "1. One using GPT-2 tokens\n",
    "2. One using BERT tokens\n",
    "\n",
    "\n",
    "- Use ONLY the TRAINING set to compute:\n",
    "  - Unigram counts\n",
    "  - Bigram counts\n",
    "\n",
    "- Use the TEST set ONLY to compute perplexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_counts(tokenized_texts):\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = defaultdict(Counter)\n",
    "    \n",
    "    # TODO: implement counting\n",
    "    for tokens in tokenized_texts:\n",
    "        for i in range(len(tokens)):\n",
    "            unigram_counts[tokens[i]] += 1\n",
    "\n",
    "            if i > 0:\n",
    "                bigram_counts[tokens[i-1]][tokens[i]] += 1\n",
    "    return unigram_counts, bigram_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(tokenized_texts, unigram_counts, bigram_counts):\n",
    "    log_prob = 0\n",
    "    N = 0\n",
    "    \n",
    "    # TODO: compute log probabilities\n",
    "    for tokens in tokenized_texts:\n",
    "        for i in range(1, len(tokens)):\n",
    "            prev_token = tokens[i-1]\n",
    "            token = tokens[i]\n",
    "\n",
    "            if unigram_counts[prev_token] > 0:\n",
    "                prob = bigram_counts[prev_token][token] / unigram_counts[prev_token]\n",
    "            else:\n",
    "                prob = 0\n",
    "\n",
    "            if prob > 0:\n",
    "                log_prob += math.log(prob)\n",
    "                N += 1\n",
    "\n",
    "    return math.exp(-log_prob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Perplexity: 8.368013047450145\n",
      "BERT Perplexity: 9.944979625785685\n"
     ]
    }
   ],
   "source": [
    "gpt2_uni, gpt2_bi = build_bigram_counts(training_gpt2)\n",
    "bert_uni, bert_bi = build_bigram_counts(training_bert)\n",
    "\n",
    "print('GPT-2 Perplexity:', compute_perplexity(test_gpt2, gpt2_uni, gpt2_bi))\n",
    "print('BERT Perplexity:', compute_perplexity(test_bert, bert_uni, bert_bi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "com6513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
